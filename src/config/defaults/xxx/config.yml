action_selector: multinomial
agent: xxx_recursive_ac
agent_model: xxx_recursive
agent_output_type: policies
agents_level2_share_params: True
agents_level3_share_params: True
xxx_agent_model_level1: xxx_recurrent_agent_level1
xxx_agent_model_level2: xxx_recurrent_agent_level2
xxx_agent_model_level3: xxx_recurrent_agent_level3
xxx_agent_level1_use_past_actions: True
xxx_agent_level2_use_past_actions: True
xxx_agent_level3_use_past_actions: True
xxx_agent_level3_use_past_actions_level1: True
xxx_critic_level1: xxx_critic_level1
xxx_critic_level2: xxx_critic_level2
xxx_critic_level3: xxx_critic_level3
xxx_critic_level1_use_past_actions: True
xxx_critic_level2_use_past_actions: True
xxx_critic_level3_use_past_actions: True
xxx_critic_level3_use_obs: True
xxx_critic_level2_use_past_actions_level1: True
xxx_critic_use_sampling: True
xxx_epsilon_start: 0.5
xxx_epsilon_finish: 0.05
xxx_epsilon_time_length: 20000
xxx_epsilon_decay_mode: "exp"
xxx_exploration_mode: "softmax"
xxx_use_entropy_regularizer: False
xxx_entropy_loss_regularization_factor: 0.000005
gamma: 0.99
learner: xxx
lr_agent: 5e-4
lr_critic: 5e-4
multiagent_controller: xxx_mac
n_critic_learner_reps: 54
obs_last_action: True
runner: xxx
target_critic_update_interval: 661500
td_lambda: 0.8
use_replay_buffer: False


