action_selector: multinomial
agent: coma_recursive_ac
agent_model: coma_recursive
agent_output_type: policies
agents_encoder_size: 64
agents_hidden_state_size: 64
batch_size: 32
batch_size_run: 32
coma_critic_sample_size: 800
coma_critic_use_sampling: True
coma_entropy_loss_regularization_factor: 5e-06
coma_epsilon_decay_mode: linear
coma_epsilon_finish: 0.05
coma_epsilon_start: 0.5
coma_epsilon_time_length: 10000
coma_exploration_mode: softmax
coma_use_entropy_regularizer: False
debug_mode: None
debug_verbose: False
env: sc2
env_stats_aggregator: sc2
env_args:
  difficulty: "3"
  episode_limit: 80
  heuristic_function: False
  measure_fps: True
  move_amount: 5
  reward_death_value: 10
  reward_negative_scale: 0.5
  reward_only_positive: True
  reward_scale: False
  reward_scale_rate: 0
  reward_win: 200
  state_last_action: True
  step_mul: 8
  map_name: 2d_3z
gamma: 0.99
learner: coma
lr_agent: 0.0005
lr_critic: 0.005
mongodb_profile: gandalf_pymarl
multiagent_controller: coma_mac
n_critic_learner_reps: 4
n_loops_per_thread_or_sub_or_main_process: 0
n_subprocesses: 0
n_threads_per_subprocess_or_main_process: 0
obs_last_action: True
observe: True
observe_db: True
run_mode: sequential
runner: coma
runner_test_batch_size: 32
save_model: False
save_model_interval: 10e6
share_agent_params: True
t_max: 5000000
target_critic_update_interval: 66150
td_lambda: 0.8
tensorboard: False
test_interval: 2000
test_nepisode: 30
use_cuda: True
use_replay_buffer: False
use_tensorboard: False