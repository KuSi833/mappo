diff --git a/src/controllers/xxx_agents.py b/src/controllers/xxx_agents.py
index da9d0d9..23c38b7 100644
--- a/src/controllers/xxx_agents.py
+++ b/src/controllers/xxx_agents.py
@@ -87,7 +87,7 @@ class XXXMultiagentController():
                                                                      rename="past_actions_level3",
                                                                      select_agent_ids=[_agent_id],
                                                                      transforms=[("shift", dict(steps=1)),
-                                                                                 ("one_hot", dict(range=(0, self.n_actions-1)))], # DEBUG!
+                                                                                 ("one_hot", dict(range=(0, self.n_actions)))], # DEBUG!
                                                                      switch=self.args.xxx_agent_level3_use_past_actions),
                                                                 dict(name="agent_id", rename="agent_id__flat", select_agent_ids=[_agent_id]),
                                                                 dict(name="xxx_epsilons_central_level3", scope="episode"),
@@ -96,7 +96,8 @@ class XXXMultiagentController():
                                                                         transforms=[("one_hot", dict(range=(0, self.n_actions - 1)))],
                                                                         switch=self.args.xxx_agent_level3_use_past_actions_level1)
                                                                    for _i in range(_n_agent_pair_samples(self.n_agents))],
-                                                                dict(name="avail_actions", select_agent_ids=[_agent_id]) ])
+                                                                dict(name="avail_actions",
+                                                                     select_agent_ids=[_agent_id])])
 
         # Set up schemes
         self.schemes = {}
@@ -228,6 +229,10 @@ class XXXMultiagentController():
         modified_inputs_list += [dict(name="policies_level3",
                                       select_agent_ids=list(range(self.n_agents)),
                                       data=self.policies_level3)]
+        modified_inputs_list += [dict(name="avail_actions",
+                                      select_agent_ids=list(range(self.n_agents)),
+                                      data=self.avail_actions_level3)]
+
         return selected_actions_list, modified_inputs_list, self.selected_actions_format
 
     def create_model(self, transition_scheme):
@@ -504,6 +509,11 @@ class XXXMultiagentController():
                 action_matrix.scatter_(0, pair_id1.squeeze(-1).view(pair_id1.shape[0],-1), actions1.squeeze(-1).view(actions1.shape[0],-1))
                 action_matrix.scatter_(0, pair_id2.squeeze(-1).view(pair_id2.shape[0],-1), actions2.squeeze(-1).view(actions2.shape[0],-1))
 
+                # extract available actions - we have to add no-op avail_actions when agents are inactive (1 only if agent is inactive)
+                avail_actions_level3 = inputs_level3["agent_input_level3"]["avail_actions"]
+
+
+                #if loss_level is None: # already correct dimensions when retrieved from episode buffer!
                 out_level3, hidden_states_level3, losses_level3, tformat_level3 = self.models["level3_0"](inputs_level3["agent_input_level3"],
                                                                                                           hidden_states=hidden_states["level3"],
                                                                                                           loss_fn=loss_fn if loss_level==3 else None,
@@ -512,8 +522,14 @@ class XXXMultiagentController():
                 if loss_level == 3:
                     return dict(losses=losses_level3), tformat_level3
 
-                # extract available actions
-                avail_actions_level3 = inputs_level3["agent_input_level3"]["avail_actions"]
+                try:
+                    active = action_matrix.view(self.n_agents, self.args.batch_size, -1).unsqueeze(2)
+                except Exception as e:
+                    pass
+                active[active==active] = 1.0
+                active[active!=active] = 0.0
+                avail_actions_level3[:,:,:,-1:] = active
+                #avail_actions_level3 = th.cat([avail_actions_level3.data, active], dim=_vdim(inputs_level3_tformat))
 
                 individual_actions, \
                 modified_inputs_level3, \
@@ -549,6 +565,7 @@ class XXXMultiagentController():
                 #self.actions_level3 = individual_actions.clone()
                 self.selected_actions_format_level3 = selected_actions_format_level3
                 self.policies_level3 = modified_inputs_level3.clone()
+                self.avail_actions_level3 = avail_actions_level3.data
 
             losses = {1:losses_level1,
                       2:losses_level2,
diff --git a/src/models/xxx.py b/src/models/xxx.py
index b5835c2..d6be069 100644
--- a/src/models/xxx.py
+++ b/src/models/xxx.py
@@ -126,7 +126,7 @@ class XXXQFunctionLevel3(nn.Module):
 
         # Set up output_shapes automatically if required
         self.output_shapes = {}
-        self.output_shapes["qvalues"] = self.n_actions # qvals
+        self.output_shapes["qvalues"] = self.n_actions + 1 # includes no-op action
         self.output_shapes.update(output_shapes)
 
         # Set up layer_args automatically if required
@@ -194,8 +194,6 @@ class XXXAdvantage(nn.Module):
 
         if baseline:
         # Fuse to XXX advantage
-            a = agent_policy.unsqueeze(1)
-            b = qvalues.unsqueeze(2)
             baseline = th.bmm(
                 agent_policy.unsqueeze(1),
                 qvalues.unsqueeze(2)).squeeze(2)
@@ -800,6 +798,14 @@ class XXXRecurrentAgentLevel2(nn.Module):
 
 class XXXNonRecurrentAgentLevel3(NonRecurrentAgent):
 
+    def __init__(self, input_shapes, n_actions, output_type=None, output_shapes={}, layer_args={}, args=None, **kwargs):
+        super().__init__(input_shapes,
+                         n_actions,
+                         output_type=output_type,
+                         output_shapes=dict(output=n_actions+1),
+                         layer_args=layer_args,
+                         args=args, **kwargs) # need to expand using no-op action
+
     def forward(self, inputs, tformat, loss_fn=None, hidden_states=None, **kwargs):
         test_mode = kwargs["test_mode"]
 
@@ -837,6 +843,14 @@ class XXXNonRecurrentAgentLevel3(NonRecurrentAgent):
 
 class XXXRecurrentAgentLevel3(RecurrentAgent):
 
+    def __init__(self, input_shapes, n_actions, output_type=None, output_shapes={}, layer_args={}, args=None, **kwargs):
+        super().__init__(input_shapes,
+                         n_actions,
+                         output_type=output_type,
+                         output_shapes=dict(output=n_actions+1),
+                         layer_args=layer_args,
+                         args=args, **kwargs) # need to expand using no-op action
+
     def forward(self, inputs, hidden_states, tformat, loss_fn=None, **kwargs):
         _check_inputs_validity(inputs, self.input_shapes, tformat)
 
diff --git a/src/runners/nstep_runner.py b/src/runners/nstep_runner.py
index bafe2c4..9647f8f 100644
--- a/src/runners/nstep_runner.py
+++ b/src/runners/nstep_runner.py
@@ -723,7 +723,7 @@ class NStepRunner():
 
         # calculate episode statistics
         self._add_episode_stats(T_env=self.T_env)
-        # a = self.episode_buffer.to_pd()
+        a = self.episode_buffer.to_pd()
         return self.episode_buffer
 
     def _add_episode_stats(self, T_env):
diff --git a/src/runners/xxx_runner.py b/src/runners/xxx_runner.py
index 718baa4..74eee87 100644
--- a/src/runners/xxx_runner.py
+++ b/src/runners/xxx_runner.py
@@ -44,7 +44,7 @@ class XXXRunner(NStepRunner):
                             dtype=np.int32,
                             missing=-1, ),
                        dict(name="avail_actions",
-                            shape=(self.n_actions,),
+                            shape=(self.n_actions+1,),
                             select_agent_ids=range(0, self.n_agents),
                             dtype=np.int32,
                             missing=-1,),
@@ -67,7 +67,7 @@ class XXXRunner(NStepRunner):
                               dtype=np.int32,
                               missing=-1,) for _i in range(_n_agent_pair_samples(self.n_agents))],
                        dict(name="policies_level3",
-                            shape=(self.n_actions,),
+                            shape=(self.n_actions+1,), # no-op action included
                             select_agent_ids=range(0, self.n_agents),
                             dtype=np.float32,
                             missing=np.nan),
@@ -266,7 +266,7 @@ class XXXRunner(NStepRunner):
             # perform environment steps and insert into transition buffer
             observations = _env.get_obs()
             state = _env.get_state()
-            avail_actions = _env.get_avail_actions()
+            avail_actions = [_aa + [0] for _aa in _env.get_avail_actions()] # add place for noop action
             ret_dict = dict(state=state)  # TODO: Check that env_info actually exists
             for _i, _obs in enumerate(observations):
                 ret_dict["observations__agent{}".format(_i)] = observations[_i]
@@ -307,7 +307,7 @@ class XXXRunner(NStepRunner):
             # perform environment steps and add to transition buffer
             observations = _env.get_obs()
             state = _env.get_state()
-            avail_actions = _env.get_avail_actions()
+            avail_actions = [_aa + [0] for _aa in _env.get_avail_actions()] # add place for noop action
             terminated = terminated
             truncated = terminated and env_info.get("episode_limit", False)
             ret_dict = dict(state=state,
